{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abuwildanm/food-recognition/blob/master/Create_Custom_Dataset_From_Google_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18KDj58LuNAM"
      },
      "source": [
        "## Rename images & categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Nd57acPWx1pS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# use this to rename the categories of the folder generated by the chrome extension\n",
        "# map_categories = {'mie_aceh - google penelusuran':'mie_aceh'}\n",
        "\n",
        "# # Rename categories\n",
        "# for before, after in map_categories.items():\n",
        "#     os.rename('./{}'.format(before), './{}'.format(after))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko0ZT7ZD3Y6a",
        "outputId": "646b9b69-723b-4694-ff62-2317dd77d9d3"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "\n",
        "print('Number of images')\n",
        "# before processing the images, locate the dataset folder for each category to dataset_no_split folder, such as dataset_no_split/mie_aceh\n",
        "categories = os.listdir('./dataset_no_split')\n",
        "print(categories)\n",
        "for cat in categories:\n",
        "    print('{}: {}'.format(cat, len(glob('dataset_no_split/{}/*'.format(cat)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IExOCCDcY-5t"
      },
      "outputs": [],
      "source": [
        "# Remove invalid images\n",
        "all_image = glob('dataset_no_split/*/*')\n",
        "for img_path in all_image:\n",
        "    if os.path.getsize(img_path) == 0 and os.path.exists(img_path):\n",
        "        os.remove(img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn28Vg0oarNc",
        "outputId": "cfd63962-5cff-41c5-924a-6fc0d18ceff6"
      },
      "outputs": [],
      "source": [
        "print('Number of valid images')\n",
        "categories = os.listdir('./dataset_no_split')\n",
        "for cat in categories:\n",
        "    print('{}: {}'.format(cat, len(glob('dataset/{}/*'.format(cat)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "crtFecvoO2pW"
      },
      "outputs": [],
      "source": [
        "# Rename each image from each category (ex: mie_aceh-1.jpg, mie_aceh-2.jpg, etc for category mie_aceh)\n",
        "for cat in categories:\n",
        "    for i, path in enumerate(sorted(glob('dataset_no_split/{}/*'.format(cat))), 1):\n",
        "        dirname = os.path.dirname(path)\n",
        "        src = path\n",
        "        dst = os.path.join(dirname, '{}-{}.jpg'.format(cat, i))\n",
        "        # Rename images\n",
        "        os.rename(src, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfm4zLO1vVmg"
      },
      "source": [
        "## Resize Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# The path of all image in dataset\n",
        "image_path = glob('dataset_no_split/*/*') # change with desired path because some of the dataset already resized\n",
        "# Resize process\n",
        "image_size = (224, 224)\n",
        "for path in image_path:\n",
        "    # Load the image from path\n",
        "    image = tf.keras.preprocessing.image.load_img(path)\n",
        "    # Resize the image\n",
        "    image = image.resize(image_size)\n",
        "    # Save the resized image\n",
        "    image.save(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rqDuADWcXVD"
      },
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cfTqzqR6YFoK"
      },
      "outputs": [],
      "source": [
        "# Make train and test directory\n",
        "os.makedirs('dataset/train', exist_ok=True)\n",
        "os.makedirs('dataset/test', exist_ok=True)\n",
        "\n",
        "# Make category directory in train and test\n",
        "for cat in categories:\n",
        "    os.makedirs('dataset/train/{}'.format(cat), exist_ok=True)\n",
        "    os.makedirs('dataset/test/{}'.format(cat), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "nV2cHnd1gbaE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "# Pick 20 percent of each category images as test set\n",
        "for cat in categories:\n",
        "    test_size = int(len(glob('dataset_no_split/{}/*'.format(cat))) * 0.2)\n",
        "    all_cat_image = glob('dataset_no_split/{}/*'.format(cat))\n",
        "    np.random.shuffle(all_cat_image)\n",
        "    for img_path in sorted(all_cat_image):\n",
        "        if len(os.listdir('dataset/test/{}'.format(cat))) <= test_size:\n",
        "            shutil.move(img_path, 'dataset/test/{}'.format(cat))\n",
        "        else:\n",
        "            shutil.move(img_path, 'dataset/train/{}'.format(cat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijwmcVeyDJaJ",
        "outputId": "f712efdf-f34f-4507-b2a9-92dde6ad6d13"
      },
      "outputs": [],
      "source": [
        "categories_split = os.listdir('./dataset/train/')\n",
        "print('Train Images')\n",
        "for cat in categories_split:\n",
        "    print('{}: {}'.format(cat, len(glob('dataset/train/{}/*'.format(cat)))))\n",
        "\n",
        "print('='*20)\n",
        "print('Test Images')\n",
        "for cat in categories_split:\n",
        "    print('{}: {}'.format(cat, len(glob('dataset/test/{}/*'.format(cat)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "VpU9OnYuROw9"
      },
      "outputs": [],
      "source": [
        "# Remove parent category directory\n",
        "for cat in categories:\n",
        "    shutil.rmtree('dataset_no_split/{}'.format(cat), ignore_errors=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNRo93MMopBLidqmcW13LF1",
      "include_colab_link": true,
      "name": "Create Custom Dataset From Google Images.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

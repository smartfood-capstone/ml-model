{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abuwildanm/food-recognition/blob/master/Create_Custom_Dataset_From_Google_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18KDj58LuNAM"
   },
   "source": [
    "## Rename images & categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nd57acPWx1pS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# use this to rename the categories of the folder generated by the chrome extension\n",
    "list_categories = os.listdir('./dataset_no_split')\n",
    "list_categories_new = [category.replace('foto makanan ', '').replace(' - Google Search', '').replace(' ', '-') for category in list_categories]\n",
    "map_categories = dict(zip(list_categories, list_categories_new))\n",
    "# Rename categories\n",
    "for before, after in map_categories.items():\n",
    "     os.rename('./dataset_no_split/{}'.format(before), './dataset_no_split/{}'.format(after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ko0ZT7ZD3Y6a",
    "outputId": "646b9b69-723b-4694-ff62-2317dd77d9d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images\n",
      "bakso: 440\n",
      "batagor: 399\n",
      "bebek-betutu: 324\n",
      "bubur-manado: 229\n",
      "gado-gado: 374\n",
      "gohu-ikan: 102\n",
      "gudeg: 290\n",
      "gulai-ikan: 479\n",
      "kerak-telor: 241\n",
      "kolak: 461\n",
      "mie-aceh: 348\n",
      "nasi-goreng: 518\n",
      "nasi-uduk: 418\n",
      "papeda: 114\n",
      "pempek: 405\n",
      "pepes-ikan: 186\n",
      "perkedel-kentang: 473\n",
      "rawon: 335\n",
      "rendang: 331\n",
      "rujak-aceh: 188\n",
      "rujak-bebek: 124\n",
      "sate: 509\n",
      "sate-bandeng: 153\n",
      "sayur-urap: 342\n",
      "semur-jengkol: 344\n",
      "sop-konro: 212\n",
      "soto: 499\n",
      "tahu-gejrot: 395\n",
      "telur-balado: 186\n",
      "tempe-bacem: 181\n",
      "tempe-goreng: 368\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "print('Number of images')\n",
    "# before processing the images, locate the dataset folder for each category to dataset_no_split folder, such as dataset_no_split/mie_aceh\n",
    "categories = os.listdir('./dataset_no_split')\n",
    "for cat in categories:\n",
    "    print('{}: {}'.format(cat, len(glob('dataset_no_split/{}/*'.format(cat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IExOCCDcY-5t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove invalid images\n",
    "all_image = glob('dataset_no_split/*/*')\n",
    "for img_path in all_image:\n",
    "    if os.path.getsize(img_path) == 0 and os.path.exists(img_path):\n",
    "        os.remove(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hn28Vg0oarNc",
    "outputId": "cfd63962-5cff-41c5-924a-6fc0d18ceff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid images\n",
      "bakso: 440\n",
      "batagor: 399\n",
      "bebek-betutu: 324\n",
      "bubur-manado: 229\n",
      "gado-gado: 374\n",
      "gohu-ikan: 102\n",
      "gudeg: 290\n",
      "gulai-ikan: 479\n",
      "kerak-telor: 241\n",
      "kolak: 461\n",
      "mie-aceh: 348\n",
      "nasi-goreng: 518\n",
      "nasi-uduk: 418\n",
      "papeda: 114\n",
      "pempek: 405\n",
      "pepes-ikan: 186\n",
      "perkedel-kentang: 473\n",
      "rawon: 335\n",
      "rendang: 331\n",
      "rujak-aceh: 188\n",
      "rujak-bebek: 124\n",
      "sate: 509\n",
      "sate-bandeng: 153\n",
      "sayur-urap: 342\n",
      "semur-jengkol: 344\n",
      "sop-konro: 212\n",
      "soto: 499\n",
      "tahu-gejrot: 395\n",
      "telur-balado: 186\n",
      "tempe-bacem: 181\n",
      "tempe-goreng: 368\n"
     ]
    }
   ],
   "source": [
    "print('Number of valid images')\n",
    "categories = os.listdir('./dataset_no_split')\n",
    "for cat in categories:\n",
    "    print('{}: {}'.format(cat, len(glob('dataset_no_split/{}/*'.format(cat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "crtFecvoO2pW"
   },
   "outputs": [],
   "source": [
    "# Rename each image from each category (ex: mie_aceh-1.jpg, mie_aceh-2.jpg, etc for category mie_aceh)\n",
    "for cat in categories:\n",
    "    for i, path in enumerate(sorted(glob('dataset_no_split/{}/*'.format(cat))), 1):\n",
    "        dirname = os.path.dirname(path)\n",
    "        src = path\n",
    "        dst = os.path.join(dirname, '{}-{}.jpg'.format(cat, i))\n",
    "        # Rename images\n",
    "        os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfm4zLO1vVmg"
   },
   "source": [
    "## Resize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The path of all image in dataset\n",
    "image_path = glob('dataset_no_split/*/*') # change with desired path because some of the dataset already resized\n",
    "# Resize process\n",
    "image_size = (224, 224)\n",
    "for path in image_path:\n",
    "    try:\n",
    "        # Load the image from path\n",
    "        image = tf.keras.preprocessing.image.load_img(path)\n",
    "        # Resize the image\n",
    "        image = image.resize(image_size)\n",
    "        # Save the resized image\n",
    "        image.save(path)\n",
    "    except:\n",
    "        # Remove the image if it is not valid\n",
    "        print('Remove {}'.format(path))\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bakso: 0\n",
      "batagor: 1\n",
      "bebek-betutu: 2\n",
      "bubur-manado: 2\n",
      "gado-gado: 0\n",
      "gohu-ikan: 9\n",
      "gudeg: 1\n",
      "gulai-ikan: 0\n",
      "kerak-telor: 2\n",
      "kolak: 0\n",
      "mie-aceh: 1\n",
      "nasi-goreng: 0\n",
      "nasi-uduk: 0\n",
      "papeda: 6\n",
      "pempek: 0\n",
      "pepes-ikan: 7\n",
      "perkedel-kentang: 0\n",
      "rawon: 3\n",
      "rendang: 0\n",
      "rujak-aceh: 4\n",
      "rujak-bebek: 4\n",
      "sate: 0\n",
      "sate-bandeng: 8\n",
      "sayur-urap: 3\n",
      "semur-jengkol: 0\n",
      "sop-konro: 2\n",
      "soto: 0\n",
      "tahu-gejrot: 0\n",
      "telur-balado: 6\n",
      "tempe-bacem: 4\n",
      "tempe-goreng: 1\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from glob import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# The path of all image in dataset\n",
    "image_path = glob('dataset_no_split/*/*')\n",
    "# The path of all category in dataset\n",
    "categories = os.listdir('./dataset_no_split')\n",
    "# The number of images in each category\n",
    "num_images = [len(glob('dataset_no_split/{}/*'.format(cat))) for cat in categories]\n",
    "\n",
    "num_augmented_images = [500 - num if num < 500 else 0 for num in num_images]\n",
    "for cat, num in zip(categories, num_augmented_images):\n",
    "    print('{}: {}'.format(cat, num))\n",
    "\n",
    "for cat, num in zip(categories, num_augmented_images):\n",
    "    # create {num} augmented images\n",
    "    all_image = glob('dataset_no_split/{}/*'.format(cat))\n",
    "    os.makedirs('dataset_no_split/augmented_{}'.format(cat), exist_ok=True)\n",
    "    for i in range(num):\n",
    "        image = random.choice(all_image)\n",
    "        image = tf.keras.preprocessing.image.load_img(image)\n",
    "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        image = image.reshape((1,) + image.shape)\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=25,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            brightness_range=[0.5, 1.2],\n",
    "            fill_mode='nearest')\n",
    "        for batch in datagen.flow(image, batch_size=1, save_to_dir='dataset_no_split/augmented_{}'.format(cat), save_prefix='augmented', save_format='jpg'):\n",
    "            break\n",
    "\n",
    "    for i, path in enumerate(sorted(glob('dataset_no_split/augmented_{}/*'.format(cat))), 1):\n",
    "        dirname = os.path.dirname(path)\n",
    "        src = path\n",
    "        dst = os.path.join(dirname, '{}-{}.jpg'.format(cat, 500 - num + i))\n",
    "        os.rename(src, dst)\n",
    "    # move the augmented image to the category folder\n",
    "    for img in glob('dataset_no_split/augmented_{}/*'.format(cat)):\n",
    "        shutil.move(img, 'dataset_no_split/{}'.format(cat))\n",
    "    shutil.rmtree('dataset_no_split/augmented_{}'.format(cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rqDuADWcXVD"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cfTqzqR6YFoK"
   },
   "outputs": [],
   "source": [
    "# Make train and test directory\n",
    "os.makedirs('dataset/train', exist_ok=True)\n",
    "os.makedirs('dataset/test', exist_ok=True)\n",
    "\n",
    "# Make category directory in train and test\n",
    "for cat in categories:\n",
    "    os.makedirs('dataset/train/{}'.format(cat), exist_ok=True)\n",
    "    os.makedirs('dataset/test/{}'.format(cat), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nV2cHnd1gbaE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# Pick 20 percent of each category images as test set\n",
    "for cat in categories:\n",
    "    test_size = int(len(glob('dataset_no_split/{}/*'.format(cat))) * 0.2)\n",
    "    all_cat_image = glob('dataset_no_split/{}/*'.format(cat))\n",
    "    np.random.shuffle(all_cat_image)\n",
    "    for img_path in all_cat_image:\n",
    "        if len(os.listdir('dataset/test/{}'.format(cat))) < test_size:\n",
    "            shutil.move(img_path, 'dataset/test/{}'.format(cat))\n",
    "        else:\n",
    "            shutil.move(img_path, 'dataset/train/{}'.format(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijwmcVeyDJaJ",
    "outputId": "f712efdf-f34f-4507-b2a9-92dde6ad6d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images\n",
      "bakso: 400\n",
      "batagor: 400\n",
      "bebek-betutu: 400\n",
      "bubur-manado: 400\n",
      "gado-gado: 400\n",
      "gohu-ikan: 400\n",
      "gudeg: 400\n",
      "gulai-ikan: 400\n",
      "kerak-telor: 400\n",
      "kolak: 400\n",
      "mie-aceh: 400\n",
      "nasi-goreng: 415\n",
      "nasi-uduk: 400\n",
      "papeda: 400\n",
      "pempek: 400\n",
      "pepes-ikan: 400\n",
      "perkedel-kentang: 400\n",
      "rawon: 400\n",
      "rendang: 400\n",
      "rujak-aceh: 400\n",
      "rujak-bebek: 400\n",
      "sate: 408\n",
      "sate-bandeng: 400\n",
      "sayur-urap: 400\n",
      "semur-jengkol: 400\n",
      "sop-konro: 400\n",
      "soto: 400\n",
      "tahu-gejrot: 400\n",
      "telur-balado: 400\n",
      "tempe-bacem: 400\n",
      "tempe-goreng: 400\n",
      "====================\n",
      "Test Images\n",
      "bakso: 100\n",
      "batagor: 100\n",
      "bebek-betutu: 100\n",
      "bubur-manado: 100\n",
      "gado-gado: 100\n",
      "gohu-ikan: 100\n",
      "gudeg: 100\n",
      "gulai-ikan: 100\n",
      "kerak-telor: 100\n",
      "kolak: 100\n",
      "mie-aceh: 100\n",
      "nasi-goreng: 103\n",
      "nasi-uduk: 100\n",
      "papeda: 100\n",
      "pempek: 100\n",
      "pepes-ikan: 100\n",
      "perkedel-kentang: 100\n",
      "rawon: 100\n",
      "rendang: 100\n",
      "rujak-aceh: 100\n",
      "rujak-bebek: 100\n",
      "sate: 101\n",
      "sate-bandeng: 100\n",
      "sayur-urap: 100\n",
      "semur-jengkol: 100\n",
      "sop-konro: 100\n",
      "soto: 100\n",
      "tahu-gejrot: 100\n",
      "telur-balado: 100\n",
      "tempe-bacem: 100\n",
      "tempe-goreng: 100\n"
     ]
    }
   ],
   "source": [
    "categories_split = os.listdir('./dataset/train/')\n",
    "print('Train Images')\n",
    "for cat in categories_split:\n",
    "    print('{}: {}'.format(cat, len(glob('dataset/train/{}/*'.format(cat)))))\n",
    "\n",
    "print('='*20)\n",
    "print('Test Images')\n",
    "for cat in categories_split:\n",
    "    print('{}: {}'.format(cat, len(glob('dataset/test/{}/*'.format(cat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VpU9OnYuROw9"
   },
   "outputs": [],
   "source": [
    "# Remove parent category directory\n",
    "for cat in categories:\n",
    "    shutil.rmtree('dataset_no_split/{}'.format(cat), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid images: 21479\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "all_image = glob('dataset/*/*/*/*')\n",
    "counter = 0\n",
    "for img_path in all_image:\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img = np.array(img)\n",
    "        if img.shape != (224, 224, 3):\n",
    "            print('Remove {}'.format(img_path))\n",
    "            os.remove(img_path)\n",
    "        else:\n",
    "            counter += 1\n",
    "    except:\n",
    "        print('Remove {}'.format(img_path))\n",
    "        os.remove(img_path)\n",
    "\n",
    "print('Number of valid images: {}'.format(counter))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRo93MMopBLidqmcW13LF1",
   "include_colab_link": true,
   "name": "Create Custom Dataset From Google Images.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abuwildanm/food-recognition/blob/master/Create_Custom_Dataset_From_Google_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18KDj58LuNAM"
   },
   "source": [
    "## Rename images & categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "Nd57acPWx1pS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# use this to rename the categories of the folder generated by the chrome extension\n",
    "list_categories = os.listdir('./dataset_no_split')\n",
    "list_categories_new = [category.replace('foto makanan ', '').replace(' - Google Penelusuran', '').replace(' ', '-') for category in list_categories]\n",
    "map_categories = dict(zip(list_categories, list_categories_new))\n",
    "# Rename categories\n",
    "for before, after in map_categories.items():\n",
    "     os.rename('./dataset_no_split/{}'.format(before), './dataset_no_split/{}'.format(after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ko0ZT7ZD3Y6a",
    "outputId": "646b9b69-723b-4694-ff62-2317dd77d9d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images\n",
      "rujak-aceh: 207\n",
      "sate-bandeng: 177\n",
      "sop-konro: 228\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "print('Number of images')\n",
    "# before processing the images, locate the dataset folder for each category to dataset_no_split folder, such as dataset_no_split/mie_aceh\n",
    "categories = os.listdir('./dataset_no_split')\n",
    "for cat in categories:\n",
    "    print('{}: {}'.format(cat, len(glob('dataset_no_split/{}/*'.format(cat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "IExOCCDcY-5t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove invalid images\n",
    "all_image = glob('dataset_no_split/*/*')\n",
    "for img_path in all_image:\n",
    "    if os.path.getsize(img_path) == 0 and os.path.exists(img_path):\n",
    "        os.remove(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hn28Vg0oarNc",
    "outputId": "cfd63962-5cff-41c5-924a-6fc0d18ceff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid images\n",
      "rujak-aceh: 207\n",
      "sate-bandeng: 177\n",
      "sop-konro: 228\n"
     ]
    }
   ],
   "source": [
    "print('Number of valid images')\n",
    "categories = os.listdir('./dataset_no_split')\n",
    "for cat in categories:\n",
    "    print('{}: {}'.format(cat, len(glob('dataset_no_split/{}/*'.format(cat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "crtFecvoO2pW"
   },
   "outputs": [],
   "source": [
    "# Rename each image from each category (ex: mie_aceh-1.jpg, mie_aceh-2.jpg, etc for category mie_aceh)\n",
    "for cat in categories:\n",
    "    for i, path in enumerate(sorted(glob('dataset_no_split/{}/*'.format(cat))), 1):\n",
    "        dirname = os.path.dirname(path)\n",
    "        src = path\n",
    "        dst = os.path.join(dirname, '{}-{}.jpg'.format(cat, i))\n",
    "        # Rename images\n",
    "        os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfm4zLO1vVmg"
   },
   "source": [
    "## Resize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The path of all image in dataset\n",
    "image_path = glob('dataset_no_split/*/*') # change with desired path because some of the dataset already resized\n",
    "# Resize process\n",
    "image_size = (224, 224)\n",
    "for path in image_path:\n",
    "    try:\n",
    "        # Load the image from path\n",
    "        image = tf.keras.preprocessing.image.load_img(path)\n",
    "        # Resize the image\n",
    "        image = image.resize(image_size)\n",
    "        # Save the resized image\n",
    "        image.save(path)\n",
    "    except:\n",
    "        # Remove the image if it is not valid\n",
    "        print('Remove {}'.format(path))\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rujak-aceh: 293\n",
      "sate-bandeng: 323\n",
      "sop-konro: 272\n"
     ]
    }
   ],
   "source": [
    "# for each category, add more augmented images so that the number of images in each category is 500\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from glob import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# The path of all image in dataset\n",
    "image_path = glob('dataset_no_split/*/*')\n",
    "# The path of all category in dataset\n",
    "categories = os.listdir('./dataset_no_split')\n",
    "# The number of images in each category\n",
    "num_images = [len(glob('dataset_no_split/{}/*'.format(cat))) for cat in categories]\n",
    "\n",
    "num_augmented_images = [500 - num if num < 500 else 0 for num in num_images]\n",
    "for cat, num in zip(categories, num_augmented_images):\n",
    "    print('{}: {}'.format(cat, num))\n",
    "\n",
    "for cat, num in zip(categories, num_augmented_images):\n",
    "    # create {num} augmented images\n",
    "    all_image = glob('dataset_no_split/{}/*'.format(cat))\n",
    "    os.makedirs('dataset_no_split/augmented_{}'.format(cat), exist_ok=True)\n",
    "    for i in range(num):\n",
    "        image = random.choice(all_image)\n",
    "        image = tf.keras.preprocessing.image.load_img(image)\n",
    "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        image = image.reshape((1,) + image.shape)\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=25,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            brightness_range=[0.5, 1.2],\n",
    "            fill_mode='nearest')\n",
    "        for batch in datagen.flow(image, batch_size=1, save_to_dir='dataset_no_split/augmented_{}'.format(cat), save_prefix='augmented', save_format='jpg'):\n",
    "            break\n",
    "\n",
    "    for i, path in enumerate(sorted(glob('dataset_no_split/augmented_{}/*'.format(cat))), 1):\n",
    "        dirname = os.path.dirname(path)\n",
    "        src = path\n",
    "        dst = os.path.join(dirname, '{}-{}.jpg'.format(cat, 500 - num + i))\n",
    "        os.rename(src, dst)\n",
    "    # move the augmented image to the category folder\n",
    "    for img in glob('dataset_no_split/augmented_{}/*'.format(cat)):\n",
    "        shutil.move(img, 'dataset_no_split/{}'.format(cat))\n",
    "    shutil.rmtree('dataset_no_split/augmented_{}'.format(cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rqDuADWcXVD"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "cfTqzqR6YFoK"
   },
   "outputs": [],
   "source": [
    "# Make train and test directory\n",
    "os.makedirs('dataset/train', exist_ok=True)\n",
    "os.makedirs('dataset/test', exist_ok=True)\n",
    "\n",
    "# Make category directory in train and test\n",
    "for cat in categories:\n",
    "    os.makedirs('dataset/train/{}'.format(cat), exist_ok=True)\n",
    "    os.makedirs('dataset/test/{}'.format(cat), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "nV2cHnd1gbaE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# Pick 20 percent of each category images as test set\n",
    "for cat in categories:\n",
    "    test_size = int(len(glob('dataset_no_split/{}/*'.format(cat))) * 0.2)\n",
    "    all_cat_image = glob('dataset_no_split/{}/*'.format(cat))\n",
    "    np.random.shuffle(all_cat_image)\n",
    "    for img_path in all_cat_image:\n",
    "        if len(os.listdir('dataset/test/{}'.format(cat))) < test_size:\n",
    "            shutil.move(img_path, 'dataset/test/{}'.format(cat))\n",
    "        else:\n",
    "            shutil.move(img_path, 'dataset/train/{}'.format(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijwmcVeyDJaJ",
    "outputId": "f712efdf-f34f-4507-b2a9-92dde6ad6d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images\n",
      "bubur-manado: 400\n",
      "gohu-ikan: 396\n",
      "papeda: 392\n",
      "rujak-aceh: 394\n",
      "rujak-bebek: 394\n",
      "sate-bandeng: 398\n",
      "sayur-urap: 398\n",
      "sop-konro: 400\n",
      "====================\n",
      "Test Images\n",
      "bubur-manado: 99\n",
      "gohu-ikan: 99\n",
      "papeda: 98\n",
      "rujak-aceh: 98\n",
      "rujak-bebek: 98\n",
      "sate-bandeng: 99\n",
      "sayur-urap: 99\n",
      "sop-konro: 99\n"
     ]
    }
   ],
   "source": [
    "categories_split = os.listdir('./dataset/train/')\n",
    "print('Train Images')\n",
    "for cat in categories_split:\n",
    "    print('{}: {}'.format(cat, len(glob('dataset/train/{}/*'.format(cat)))))\n",
    "\n",
    "print('='*20)\n",
    "print('Test Images')\n",
    "for cat in categories_split:\n",
    "    print('{}: {}'.format(cat, len(glob('dataset/test/{}/*'.format(cat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "VpU9OnYuROw9"
   },
   "outputs": [],
   "source": [
    "# Remove parent category directory\n",
    "for cat in categories:\n",
    "    shutil.rmtree('dataset_no_split/{}'.format(cat), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid images: 12851\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "all_image = glob('dataset/*/*/*/*')\n",
    "counter = 0\n",
    "for img_path in all_image:\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img = np.array(img)\n",
    "        if img.shape != (224, 224, 3):\n",
    "            print('Remove {}'.format(img_path))\n",
    "            os.remove(img_path)\n",
    "        else:\n",
    "            counter += 1\n",
    "    except:\n",
    "        print('Remove {}'.format(img_path))\n",
    "        os.remove(img_path)\n",
    "\n",
    "print('Number of valid images: {}'.format(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRo93MMopBLidqmcW13LF1",
   "include_colab_link": true,
   "name": "Create Custom Dataset From Google Images.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
